{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install git+https://github.com/Simoniman/goes_api.git\n",
    "! pip install git+https://github.com/Simoniman/himawari_api.git\n",
    "! wget -N -O utils.py 'https://github.com/Simoniman/GeoSatStitch/raw/main/utils.py' \n",
    "! wget -N -O Himawari_Download.py 'https://github.com/Simoniman/GeoSatStitch/raw/main/Himawari_ahi/Himawari_Download.py'\n",
    "! wget -N -O himawari_utils.py 'https://github.com/Simoniman/GeoSatStitch/raw/main/Himawari_ahi/himawari_utils.py'\n",
    "! pip install patool\n",
    "! pip install satpy\n",
    "! pip install rasterio\n",
    "! pip install matplotlib\n",
    "! pip install cartopy \n",
    "! pip install eumdac\n",
    "! pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import shutil\n",
    "from glob import glob\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import eumdac\n",
    "import patoolib\n",
    "from satpy import Scene, MultiScene, DataQuery\n",
    "from pyresample import create_area_def\n",
    "from utils import find_format, find_reader, geo_area_def, extract_zip_files, read_credentials_from_config\n",
    "import goes_api\n",
    "from goes_api import download_latest_files\n",
    "import himawari_api\n",
    "from himawari_api import download_latest_files\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "import threading\n",
    "from himawari_utils import *\n",
    "from google.colab import drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('/content/drive/'):\n",
    "    drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### download seviri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key, consumer_secret = read_credentials_from_config(filename='./personal_config.json')\n",
    "\n",
    "credentials = (consumer_key, consumer_secret)\n",
    "\n",
    "token = eumdac.AccessToken(credentials)\n",
    "\n",
    "seviri_pdcid = \"EO:EUM:DAT:MSG:HRSEVIRI\" # prime data collection id\n",
    "seviri_iodcid = \"EO:EUM:DAT:MSG:HRSEVIRI-IODC\" # indian ocean data collection id\n",
    "\n",
    "datastore = eumdac.DataStore(token)\n",
    "seviri_pdc = datastore.get_collection(seviri_pdcid) # prime data collection\n",
    "seviri_iodc = datastore.get_collection(seviri_iodcid) # indian ocean data collection\n",
    "\n",
    "seviri_pp_latest = seviri_pdc.search().first() # prime product_latest\n",
    "seviri_iop_latest = seviri_iodc.search().first() # indian ocean product_latest\n",
    "products = [seviri_pp_latest, seviri_iop_latest]\n",
    "\n",
    "# Create the \"tmp\" directory if it doesn't exist\n",
    "base_dir = './tmp/Meteosat_seviri'\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "for count, product in enumerate(products):\n",
    "    if os.path.exists(os.path.join(base_dir, str(count))):\n",
    "      # If it exists, remove it and its contents\n",
    "      shutil.rmtree(os.path.join(base_dir, str(count)))\n",
    "    os.makedirs(os.path.join(base_dir, str(count)), exist_ok=True)\n",
    "    try:\n",
    "        with product.open() as fsrc, open(os.path.join(base_dir,str(count), os.path.basename(fsrc.name)), mode='wb') as fdst:\n",
    "            shutil.copyfileobj(fsrc, fdst)\n",
    "            print(f'Download of product {product} finished.')\n",
    "            extract_zip_files(os.path.join(base_dir, str(count)))\n",
    "    except eumdac.product.ProductError as error:\n",
    "        print(f\"Error related to the product '{product}' while trying to download it: '{error.msg}'\")\n",
    "    except requests.exceptions.ConnectionError as error:\n",
    "        print(f\"Error related to the connection: '{error.msg}'\")\n",
    "    except requests.exceptions.RequestException as error:\n",
    "        print(f\"Unexpected error: {error}\")\n",
    "\n",
    "print('All downloads are finished.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### download goes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Define protocol and local directory\n",
    "base_dir = \"./tmp/\"\n",
    "protocol = \"s3\"\n",
    "fs_args = {}\n",
    "\n",
    "#### Define satellite, sensor, product_level and product\n",
    "satellite = [\"16\",\"18\"] # 16 (Goes 16) ==> EAST  /// 18 (Goes 18) ==> WEST\n",
    "sensor = \"ABI\"\n",
    "product_level = \"L1B\"\n",
    "product = \"Rad\"\n",
    "\n",
    "#### Define sector and filtering options\n",
    "sector = \"F\"\n",
    "scene_abbr = None  # DO NOT SPECIFY FOR FULL DISC SECTOR\n",
    "scan_modes = None  # select all scan modes (M3, M4, M6)\n",
    "channels = None  # select all channels\n",
    "channels = [\"C13\"]  # select channels subset\n",
    "filter_parameters = {}\n",
    "filter_parameters[\"scan_modes\"] = scan_modes\n",
    "filter_parameters[\"channels\"] = channels\n",
    "filter_parameters[\"scene_abbr\"] = scene_abbr\n",
    "\n",
    "#### Downloading options\n",
    "n_threads = 20  # n_parallel downloads\n",
    "force_download = False  # whether to overwrite existing data on disk\n",
    "look_ahead_minutes = 60\n",
    "N = 1\n",
    "operational_checks = True\n",
    "\n",
    "# Create the base_dir if it doesn't already exist\n",
    "if not os.path.exists(base_dir):\n",
    "    os.makedirs(base_dir)\n",
    "    print(f\"Directory '{base_dir}' created successfully.\")\n",
    "else:\n",
    "    print(f\"Directory '{base_dir}' already exists.\")\n",
    "\n",
    "\n",
    "if type(satellite) == list:\n",
    "  for sat in satellite : \n",
    "    fpaths = goes_api.download_latest_files(\n",
    "        N=N,\n",
    "        base_dir=base_dir,\n",
    "        protocol=protocol,\n",
    "        fs_args=fs_args,\n",
    "        satellite=sat,\n",
    "        sensor=sensor,\n",
    "        product_level=product_level,\n",
    "        product=product,\n",
    "        sector=sector,\n",
    "        filter_parameters=filter_parameters,\n",
    "        n_threads=n_threads,\n",
    "        force_download=force_download,\n",
    "        check_data_integrity=True,\n",
    "        progress_bar=True,\n",
    "        verbose=True,\n",
    "        look_ahead_minutes=look_ahead_minutes,\n",
    "    )\n",
    "\n",
    "else:\n",
    "    fpaths = goes_api.download_latest_files(\n",
    "        N=N,\n",
    "        base_dir=base_dir,\n",
    "        protocol=protocol,\n",
    "        fs_args=fs_args,\n",
    "        satellite=satellite,\n",
    "        sensor=sensor,\n",
    "        product_level=product_level,\n",
    "        product=product,\n",
    "        sector=sector,\n",
    "        filter_parameters=filter_parameters,\n",
    "        n_threads=n_threads,\n",
    "        force_download=force_download,\n",
    "        check_data_integrity=True,\n",
    "        progress_bar=True,\n",
    "        verbose=True,\n",
    "        look_ahead_minutes=look_ahead_minutes,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### download himawari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_path = 'Himawari_Download.py' \n",
    "base_dir = \"./tmp/\"\n",
    "channels = [\"C13\"]\n",
    "gradual_unzip = True # False for skipping unzipping and 1 for unzipping\n",
    "counter_list = []\n",
    "\n",
    "# NOTE : in this case, the gradual_unzip argument must be True. \n",
    "# Create a threading Thread\n",
    "t = threading.Thread(target=run_script_and_check, args=(script_path, base_dir, channels, True, counter_list))\n",
    "\n",
    "# Start the thread\n",
    "t.start()\n",
    "\n",
    "# We should wait until first run of the main download function. using a wait function over a global list is much more efficient and reliable that waiting a specific amount of time using time.sleep function.\n",
    "wait_function(counter_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Goes-18 scene & Goes-16 scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = glob('./tmp/**/*.nc', recursive=True)\n",
    "\n",
    "#G18\n",
    "g18_idx = ['G18' in fn for fn in filenames].index(True)\n",
    "g18_scn = Scene(reader='abi_l1b',filenames=[filenames[g18_idx]])\n",
    "g18_scn.load(['C13'])\n",
    "\n",
    "#G16\n",
    "g16_idx = ['G16' in fn for fn in filenames].index(True)\n",
    "g16_scn = Scene(reader='abi_l1b',filenames=[filenames[g16_idx]])\n",
    "g16_scn.load(['C13'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MSG-3 (Prime) scene & MSG-2 (Indian Ocean) scene "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = glob('./tmp/**/*.nat', recursive=True)\n",
    "\n",
    "#MSG3\n",
    "msg3_idx = ['MSG3' in fn for fn in filenames].index(True)\n",
    "msg3_scn = Scene(reader='seviri_l1b_native', filenames=[filenames[msg3_idx]])\n",
    "msg3_scn.load(['IR_108'])\n",
    "\n",
    "#MSG2\n",
    "msg2_idx = ['MSG2' in fn for fn in filenames].index(True)\n",
    "msg2_scn = Scene(reader='seviri_l1b_native', filenames=[filenames[msg2_idx]])\n",
    "msg2_scn.load(['IR_108'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Himawari scene\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = glob('./tmp/**/*.DAT', recursive=True)\n",
    "\n",
    "him_scn = Scene(reader='ahi_hsd', filenames=filenames)\n",
    "him_scn.load(['B13'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create Multiscene object and resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating multiscene from list of scenes\n",
    "scenes = [g18_scn, g16_scn, msg3_scn, msg2_scn, him_scn]\n",
    "mscn = MultiScene(scenes)\n",
    "\n",
    "# grouping\n",
    "groups = { DataQuery(name='IR_group', wavelength=(10, 11, 12)) : ['B13', 'C13', 'IR_108'] }\n",
    "mscn.group(groups)\n",
    "\n",
    "# area definition\n",
    "dst_area_def = create_area_def('world_area', 3857, \n",
    "                            # resolution=xr.DataArray([2000, 2000], attrs={\"units\": \"meters\"}),\n",
    "                            shape=(4000, 5000), # (12515, 20037)\n",
    "                            area_extent= [-180 + 1/1e9, -74, 180, 74], units='degrees')\n",
    "\n",
    "# resampling\n",
    "resampled = mscn.resample(dst_area_def, reduce_data=False, radius_of_influence=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without applying weight :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blended_without_weights = resampled.blend() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With applying weights :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from satpy.modifiers.angles import get_satellite_zenith_angle,_get_sensor_angles\n",
    "import pyresample\n",
    "from pyresample import image, geometry\n",
    "def _csatz_resamp_nparr(scene, band, area_def):\n",
    "    # satz in geos coordinate system\n",
    "    sata, satz = _get_sensor_angles(scene[band])\n",
    "    csatz = np.cos(np.deg2rad(satz.to_numpy()))\n",
    "    # we need to specify source and destination projection systems\n",
    "    src_areadef = scene[band].attrs['area']\n",
    "    dst_areadef = area_def\n",
    "    # resample satz from source coordinate projection system to destination projection system\n",
    "    csatz_con = image.ImageContainerNearest(csatz, src_areadef, radius_of_influence=50000) #pyresample.imagecontainer\n",
    "    csatz_resamp_con = csatz_con.resample(dst_areadef) #pyresample.imagecontainer\n",
    "    csatz_resamp_nparr = csatz_resamp_con.image_data #numpy.array\n",
    "    return csatz_resamp_nparr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdrive_cashed_folder_name = 'cos_sat_zenith_3000-5000'\n",
    "cache_path_g18 = f'./drive/MyDrive/{gdrive_cashed_folder_name}/csatz_resamp_nparr_g18.npy'\n",
    "if os.path.isfile(cache_path_g18):\n",
    "    csatz_resamp_nparr_g18 = np.load(cache_path_g18)\n",
    "else:\n",
    "    csatz_resamp_nparr_g18 = _csatz_resamp_nparr(scene=g18_scn, band='C13', area_def=dst_area_def)\n",
    "    np.save(cache_path_g18, csatz_resamp_nparr_g18)\n",
    "\n",
    "cache_path_g16 = f'./drive/MyDrive/{gdrive_cashed_folder_name}/csatz_resamp_nparr_g16.npy'\n",
    "if os.path.isfile(cache_path_g16):\n",
    "    csatz_resamp_nparr_g16 = np.load(cache_path_g16)\n",
    "else:\n",
    "    csatz_resamp_nparr_g16 = _csatz_resamp_nparr(scene=g16_scn, band='C13', area_def=dst_area_def)\n",
    "    np.save(cache_path_g16, csatz_resamp_nparr_g16)\n",
    "\n",
    "cache_path_msg3 = f'./drive/MyDrive/{gdrive_cashed_folder_name}/csatz_resamp_nparr_msg3.npy'\n",
    "if os.path.isfile(cache_path_msg3):\n",
    "    csatz_resamp_nparr_msg3 = np.load(cache_path_msg3)\n",
    "else:\n",
    "    csatz_resamp_nparr_msg3 = _csatz_resamp_nparr(scene=msg3_scn, band='IR_108', area_def=dst_area_def)\n",
    "    np.save(cache_path_msg3, csatz_resamp_nparr_msg3)\n",
    "\n",
    "cache_path_msg2 = f'./drive/MyDrive/{gdrive_cashed_folder_name}/csatz_resamp_nparr_msg2.npy'\n",
    "if os.path.isfile(cache_path_msg2):\n",
    "    csatz_resamp_nparr_msg2 = np.load(cache_path_msg2)\n",
    "else:\n",
    "    csatz_resamp_nparr_msg2 = _csatz_resamp_nparr(scene=msg2_scn, band='IR_108', area_def=dst_area_def)\n",
    "    np.save(cache_path_msg2, csatz_resamp_nparr_msg2)\n",
    "\n",
    "cache_path_him = f'./drive/MyDrive/{gdrive_cashed_folder_name}/csatz_resamp_nparr_him.npy'\n",
    "if os.path.isfile(cache_path_him):\n",
    "    csatz_resamp_nparr_him = np.load(cache_path_him)\n",
    "else:\n",
    "    csatz_resamp_nparr_him = _csatz_resamp_nparr(scene=him_scn, band='B13', area_def=dst_area_def)\n",
    "    np.save(cache_path_him, csatz_resamp_nparr_him)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(csatz_resamp_nparr_msg3)\n",
    "plt.colorbar()\n",
    "\n",
    "# it is possible to apply some more modifications to the weight function if required\n",
    "# plt.figure()\n",
    "# weight_log = np.log10(csatz_resamp_nparr_g16)+0.5\n",
    "# weight_log_clipped = np.clip(x,-4,0)\n",
    "# plt.imshow((weight_log_clipped+4)/4)\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from satpy.multiscene import stack\n",
    "from functools import partial\n",
    "weights = [csatz_resamp_nparr_g18, csatz_resamp_nparr_g16, csatz_resamp_nparr_msg3, csatz_resamp_nparr_msg2, csatz_resamp_nparr_him]\n",
    "stack_with_weights = partial(stack, weights=weights)\n",
    "blended_with_weights = resampled.blend(blend_function=stack_with_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blended = blended_with_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = 'IR_group'\n",
    "crs = blended[channel].attrs['area'].to_cartopy_crs()\n",
    "plt.figure(figsize=(20,12))\n",
    "\n",
    "ax = plt.axes(projection=crs)\n",
    "\n",
    "cmap = 'Greys'  # Example colormap without transparency\n",
    "\n",
    "dataset = blended[channel]\n",
    "dataset.plot.imshow(transform=crs, cmap=cmap)\n",
    "ax.coastlines()\n",
    "ax.gridlines(draw_labels=True, linewidth=1, color='gray', alpha=0.5, linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or if you want to visualize in another software, you can save the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blended.save_datasets()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geosat_down",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
